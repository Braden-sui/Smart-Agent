# --- Local LLM Configuration ---
# Select the backend for the local LLM.
# Options: "rwkv7-gguf", "transformers", "mock"
LLM_BACKEND=rwkv7-gguf

# --- Configuration for 'rwkv7-gguf' backend ---
# Full path to your downloaded GGUF model file.
# Example: ./models/rwkv-v7-2.9b-g1-f16.gguf
# Ensure this file exists at the specified path.
RWKV_MODEL_PATH=./models/your_downloaded_rwkv_gguf_model.gguf

# Context window size for the GGUF model (e.g., 2048, 4096, 8192).
# Higher values consume more RAM but allow for longer conversation history.
LLM_CONTEXT_SIZE=4096

# Path to store and load conversation history for the RWKV7GGUFClient.
# The directory will be created if it doesn't exist.
CONVERSATION_STATE_PATH=./companion_memory/rwkv_conversation_history.json

# --- Configuration for 'transformers' backend (if LLM_BACKEND is set to "transformers") ---
# Model name or path for Hugging Face Transformers (e.g., "microsoft/DialoGPT-small")
# LLM_MODEL_NAME=microsoft/DialoGPT-small

# --- Memory Configuration (Mem0) ---
MEM0_API_KEY=your_api_key_here  # Optional, falls back to in-memory if not set or if connection fails
 path

# Memory Configuration
MEM0_API_KEY=your_api_key_here  # Optional, falls back to in-memory
